---
title: "Coursera Data Science Keystone Report 1"
author: "Frederic Auberson"
date: "February 6, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
library(Matrix)
library(quanteda)
library(readtext)
#install readtext using devtools::install_github("kbenoit/readtext")
library(RColorBrewer)
```

## Introduction

This is the first milestone report for the Coursera Data Science Keystone project. We will be using the HC Corpora dataset (See the readme file at http://www.corpora.heliohost.org/aboutcorpus.html for details), and perform some analysis on it to explore Natural Language Processing techniques. To do this, we will be using the Quanteda package (https://cran.r-project.org/web/packages/quanteda).

## Data Download and Preparation

As instructed, the dataset is loaded from the Coursera site (and not from its original location at Heliohost). Let's examine the size of the files we will be working with:

```{r download, results=TRUE}
if (!file.exists("final")){
  URL <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
  download.file(url = URL, destfile = "Coursera-SwiftKey.zip", quiet = TRUE)
  unzip("Coursera-SwiftKey.zip")
  file.remove("Coursera-SwiftKey.zip")
}

file.info(list.files("final/en_US", full.names = TRUE))[,c(1,5)]
```

The dataset is quite large. Attempting to analyse it in whole brought my computer to its knee, despite it being more than generously sized (8 cores and 32GB RAM). Being rather stubborn, I persevered for a while, trying a number of packages (tm, ngram, quanteda, text2vec and more) in the hope that one would be up to the task. I realised much too late that a subset of the data would be more than sufficient to conduct this first analysis, since we can assume the results gathered from a representative sample to be applicable to the whole dataset.

For this reason, we will perform the analysis on a subset of the data: We will use a similar directory structure as that used with the original data, but which contains only a random 1% of the original number of lines. The code used to generate these files has been moved, for conciseness' sake, to Appendix A, and is not required to follow this analysis, as it can be conducted in the same manner on the complete dataset, if sufficient resources are available.

```{r sample, echo=FALSE, results=TRUE}
samplePercent <- 1 # Percentage of lines to keep. Adjust this value as needed.

makeSample <- function(src, dest, filename) {
  setwd(src)
  connection <- file(description=filename, open="rb")
  contents <- readLines(connection, skipNul = TRUE, encoding = "UTF-8")
  subsample <- sample(contents, size=length(contents)*(samplePercent/100), replace=FALSE)
  close(connection)
  setwd(dest)
  write(subsample, filename, sep="\n")
}

if (samplePercent!=100) {
  src <- getwd()
  dest <- tempdir()

  dir.create(paste(dest, "final", "en_US", sep = "/"), recursive = TRUE)

  makeSample(src, dest, "final/en_US/en_US.blogs.txt")
  makeSample(src, dest, "final/en_US/en_US.news.txt")
  makeSample(src, dest, "final/en_US/en_US.twitter.txt")

  setwd(dest)
}
```

The stubbornness paid off in a way, since I have gained a good overview for the packages that are available for the type of NLP tasks we're interested in. The Quanteda package, in my opinion, struck the best balance between performance on the one hand, and useability and versatility on the other. For this reason, I've decided to use Quanteda for this report, and possibly for the rest of the Keystone project. 

The sampled data is first read into a quanteda corpus. All files in the English (en_US) directory are read using the readtext package, taking care to select the right characterset. The file name is split into metadata fields, the first part of the file name being the ISO-3166 locale of the document (e.g. "en_US"), the second indicating the actual content (e.g. "tweets"):

```{r corpus, results=TRUE}
texts <- readtext("final/en_US/*.txt", docvarsfrom="filenames", dvsep="\\.", docvarnames=c("lang","name"), encoding="UTF-8", verbosity=0)
c <- corpus(texts)
saveRDS(c, file="~/corpus.rds") #DELETE THIS
summary(c) 
```

Note that if the subsampling step was to be skipped, for example by setting the sampling parameter to 100%, or by omitting the sampling step detailed in Appendix A, the above code chunk would process the original files in full: The sampled files have been generated so as to follow the same naming and directory structure as the originals, so that the rest of the code remains unchanged.

## Cleanup and exploratory analysis

The data first needs to be filtered: Our end goal is building an engine for predicting the next word during text entry. For this reason, we will neither need punctuation, nor numbers: We will not, later on, attempt to predict punctuation, and predicting the numbers in a sentence would be futile (as they will tend to depend wholly on context). For the same reason, we will remove symbols, twitter-specific handles, and URLs. 

For a typical analysis natural language, we would do two filtering steps we will not perform this time: 
- First, we would remove stopwords, which are often-used words that give little information about the content of a phrase (I, but, it's, we, our...). These word will be kept in this study, as they are word that our prediction engine will try to predict also.
- Second, we would typically perform stemming, which is the reduction of words to their root ("cooking" and "cooks" become "cook") in order to perform any statistical analysis on the roots rather than on the variants. This will also be left out, since we intend to predict the whole word. Performing stemming in our prediction engine would limit the predictions to the root of the words, which would limit its usefulness (though it would still speed up writing, and so would retain a certain usefulness).

Lastly, we will convert the characters to Lowercase. This decision is discutable, and would probably be inadequate for languages that use extensive capitalisation such as German, but I believe it is a reasonable choice for English or, say, French. In a real-world scenario, we would probably want to correct the cases, in order to predict proper nouns or other classes of words that require capitalisation correctly. This step is intentionally left out as a simplification, all words will be analysed as lowercase. 

Quanteda's terse syntax allows us to pack all this preprocessing in the same call as the one that calculates the number of occurence of each words (or, later, of each N-gram). Due to the large number of parameters, and in order to ensure we are using the same parameters later on, we will wrap this call in a function, and call this function to get a first impression of the frequency with which each word occurs:

```{r dfm, results=TRUE}
getDFM <- function(...) {
  x <- dfm(c, stem=FALSE, tolower=TRUE, removeNumbers=TRUE, removePunct=TRUE, removeSymbols=TRUE, removeHyphens=FALSE, removeTwitter=TRUE, removeURL=TRUE, ...)
  dfm_trim(x, min_count=2)
}
dfm <- getDFM()
```

Also, to save memory, we're removing any item that appears only once (using Quanteda's dfm_trim function). The result is called a Document Feature Matrix (DFM), which lists the occurence of the different terms in each document. To get a first impression, let's plot the number of occurences of the to 30 words:

```{r topfeatures_barplot, results=TRUE}
colScheme<-brewer.pal(n = 9, "Set1")
par(las=2, cex.axis=.7, mar=c(5,5,1,1))
barplot(rev(topfeatures(dfm, 30)), col=rep(colScheme[1:2], 15), horiz=TRUE)
```

Unsurprisingly, the words that occur the most often are the stopwords we elected not to remove from the analysis. The rest is as expected: common words, in lowercase, without punctuation or special symbols. 

Quanteda gives us a convenient way to express this in a much more compact and appealing way by wrapping the "wordcloud" package. Generating a wordcloud from our DFM in Quanteda is a simple one-liner:

```{r wordcloud, results=TRUE, fig.width=4, fig.height=4}
par(mar=c(1,1,1,1))
textplot_wordcloud(dfm, max.words=200, colors=colScheme)
```

Again, the words which stand out are as expected. This plot type allows us to fit significantly more words on less space, but nothing in the 200 words displayed seems out of order or unexpected. As a last check, let's sample some words to get an impression whether less commonly used words also look normal: 

```{r random_features, results=TRUE}
set.seed(123)
featnames(dfm_sample(dfm, 100, replace=FALSE, what="features"))
```

The result looks fine, all words are roughly in the form we expect.

Quanteda allows us to perform much much advanced analysis on our textual data. The following example compares the lexical diversity of the different sources we have in our corpus:

```{r textstat_lexdiv, results=TRUE,  fig.width=7, fig.height=3}
par(las=2, cex.axis=.75,  mgp=c(3,1,0), mar=c(5,7,1,1))
barplot(textstat_lexdiv(dfm, "CTTR"), pch = 16, horiz=TRUE, xlab = "Lexical Diversity (Carroll Type-Token Ratio (CTTR))", col=colScheme[1:3])
```

As expected, the diversity in vocabulary found in text written by journalists is significantly higher than that found in blogs or twitter messages. But rather than continue sampling Quanteda's many possibilities, we'll have a more in-depth look at the generation of n-grams, since these will be the basis on which we build our text prediction engine. 

## Analysis of N-Gram results

An N-Gram is a sequence of words that actually appears in a corpus. An approach in building a text prediction engine would be to extract N-Grams from our corpus, and count the number of occurences of each. Then, when text is entered into our application, we would search for N-Grams that begins with the N-1 words last typed, and select the N-Gram that occurs the most often: The Nth word in that N-Gram is the most likely candidate, and we would propose that word to the user as the predicted word.

We actually have already computed the number of occurences for a special case of N-Grams: The Document Feature Matrix (DFM) we have calculated returns the number of occurence of each word, and each word can be considered as being the trivial case of an N-Gram with N=1 -- a 1-Gram, or Unigram. Higher-order N-Grams can be calculated using the same method, which we had declated while calculating our DFM:

```{r unigram, results=TRUE}
unigram <- dfm #No need to compute this a second time!
```
```{r bigram, results=TRUE}
bigram <- getDFM(ngrams=2)
```
```{r trigram, results=TRUE}
trigram <- getDFM(ngrams=3)
```
```{r quadrigram, results=TRUE}
quadrigram <- getDFM(ngrams=4)
```

We can, as before, generate charts showing the number of occurence of each N-Gram:

```{r all_ngrams, results=TRUE, fig.width=8, fig.height=8}
par(las=2, cex.axis=.7, mar=c(5,8,1,1))
topNgrams<-rev(c(topfeatures(unigram, 10), topfeatures(bigram, 10), topfeatures(trigram, 10), topfeatures(quadrigram, 10)))
colors<-rev(c(rep(colScheme[1:2], 5),rep(colScheme[3:4], 5),rep(colScheme[5:6], 5),rep(colScheme[7:8], 5)))
barplot(topNgrams, col=colors, horiz=TRUE, log="x")
```

## Next steps: Developing a predictive model

The N-Grams give us the model we need to build a prediction engine. The value of N itself is a tradeoff between prediction accuracy and resources consumed: A quadrigram would deliver excellent results, since it bases its prediction on a lot of context, but it would consume an unreasonable amount of memory (over 16GB in the small 1% sample used here). A bigram would be the smallest structure useable to conduct predictions, but it would provide inaccurate results, since a prediction would be based on the most likely term to follow a single word. A trigram might be a reasonable tradeoff.

Once N has been chosen, the prediction mechanism can be built. One simple method is to scan the list of known N-Grams in search of the last N-1 words typed, and return the Nth word in the trigram that occurs the most frequently. 

The list of N-Grams is potentially large, and scanning the list may end up being computationally intensive, in particular considering it has to be done every time the input changes. For that reason, it is advisable to store the N-Grams in a structure that allow an efficient lookup to be performed: A hash table. There are a number of ways to compute Hash tables in R, the native one being using an Environment object (See https://www.r-bloggers.com/hash-table-performance-in-r-part-i/ for details). Another one is by using specialised packages, such as hashmap.

But possibly even more efficient than implementing our own mechanism for prediction is using one that is already implemented. Before implementing my own solution, I intend to check the existing R packages for functionality that can be used instead.

## Appendix A: Simple method to sample the data

For reference, the method used to sample the data is as follows. It reads the files provided in the final/en_US subdirectory, and creates sampled files in a similar directory structure unter the host's temp directory, then changed the working directory to that directory. This means that any subsequent code may run on the sampled data in exactly the same way as if it was running on the original data.

```{r eval=FALSE}
samplePercent <- 1 # Percentage of lines to keep. Adjust this value as needed.

makeSample <- function(src, dest, filename) {
  setwd(src)
  connection <- file(description=filename, open="rb")
  contents <- readLines(connection, skipNul = TRUE, encoding = "UTF-8")
  subsample <- sample(contents, size=length(contents)*(samplePercent/100), replace=FALSE)
  close(connection)
  setwd(dest)
  write(subsample, filename, sep="\n")
}

if (samplePercent!=100) {
  src <- getwd()
  dest <- tempdir()

  dir.create(paste(dest, "final", "en_US", sep = "/"), recursive = TRUE)

  makeSample(src, dest, "final/en_US/en_US.blogs.txt")
  makeSample(src, dest, "final/en_US/en_US.news.txt")
  makeSample(src, dest, "final/en_US/en_US.twitter.txt")

  setwd(dest)
```